---
title: 'brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R'
authors: "Nicholas Tierney, Dianne Cook, Tania Prvan"
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
  pdf_document:
    keep_tex: yes
---

<!-- --- -->
<!-- title: "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R" -->
<!-- author: -->
<!--   # see ?rjournal_article for more information -->
<!--   - name: Nicholas Tierney -->
<!--     affiliation: Monash University -->
<!--     address: -->
<!--     - Monash Clayton -->
<!--     url: https://njtierney.com -->
<!--     orcid: 0000-0002-9079-593X -->
<!--     email:  nicholas.tierney@gmail.com -->
<!--   - name: Dianne Cook -->
<!--     url: https://dicook.org -->
<!--     email: dicook@monash.edu -->
<!--     orcid: 0000-0002-9079-593X -->
<!--     affiliation: Monash University -->
<!--     address: -->
<!--     - Monash Clayton -->
<!--     affiliation2: ACEMS -->
<!--     address2: -->
<!--     - ACEMS Brisbane -->
<!-- abstract: > -->
<!--   An abstract of less than 150 words. -->
<!-- preamble: | -->
<!--   % Any extra LaTeX you need in the preamble -->

<!-- # per R journal requirement, the bib filename should be the same as the output  -->
<!-- # tex file. Don't forget to rename the bib file and change this example value. -->
<!-- bibliography: brolgar-paper.bib -->
<!-- output: rticles::rjournal_article -->
<!-- --- -->


```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message = FALSE, echo = FALSE}
library(tidyverse)
library(kableExtra)
library(lme4)
library(modelr)
library(tsibble)
library(brolgar)
library(patchwork)
library(gridExtra)
library(conflicted)
# remotes::install_github("rensa/stickylabeller")
library(stickylabeller)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("pull", "dplyr")
conflict_prefer("heights", "brolgar")
# the
heights <- brolgar::heights %>% filter(year > 1700)
# because we are exploring the temporal patterns, we cannot reliably say anything about those individuals with less than 5 measurements
# NOTE - how to discuss that the data has been filtered from original source?
```

# Abstract

Longitudinal (panel) data provide the opportunity to examine temporal patterns of individuals, because measurements are collected on the same person at different, and often irregular, time points. The data is typically visualised using a "spaghetti plot", where a line plot is drawn for each individual. When overlaid in one plot, it can have the appearance of a bowl of spaghetti. With even a small number of subjects, these plots are too overloaded to be read easily. The interesting aspects of individual differences are lost in the noise. Longitudinal data is often modelled with a hierarchical linear model to capture the overall trends, and variation among individuals, while accounting for various levels of dependence. However, these models can be difficult to fit, and can miss unusual individual patterns. Better visual tools can help to diagnose longitudinal models, and better capture the individual experiences. This paper introduces the R package, brolgar (BRowse over Longitudinal data Graphically and Analytically in R), which provides tools to identify and summarise interesting individual patterns in longitudinal data.

# Introduction

This paper is about exploring longitudinal data effectively. Longitudinal data can be defined as individuals repeatedly measured through time, and its inherent structure allows us to examine temporal patterns of individuals. This structure is shown in Figure \@ref(fig:heights-sample-plot), which shows a sample of data from the average height of Australian males. The individual component is `country`, and the time component is `year`. The variable `country` is measured repeatedly from 1900 to 1970, with irregular time periods between years.


```{r heights-sample-plot, fig.cap = "A sample of average heights of men in Australia for 1900-1970. The heights increase over time, but are measured at irregular intervals.", echo = FALSE}

heights_oz <- heights %>% 
  filter(country == "Australia") %>% 
  slice_tail(n = 5) 

mytheme <- gridExtra::ttheme_default(
  core = list(padding = unit(c(5, 10), "mm"))
  )

tbl_heights_oz <- heights_oz %>% 
  mutate(height_cm = round(height_cm, 1)) %>% 
  tableGrob(theme = mytheme, 
            rows = NULL)

gg_heights_oz <- ggplot(heights_oz,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_point() + 
  geom_line()
# NOTE - add time points for each measurement?

grid.arrange(tbl_heights_oz,
             gg_heights_oz,
             nrow = 1,
             as.table = TRUE)
```

The full dataset of Figure \@ref(fig:heights-sample-plot) is shown in Figure \@ref(fig:spaghetti), showing 144 countries from the year 1700. This plot is challenging to understand because there is overplotting, making it hard to see the individuals. Solutions to this aren't always obvious. Showing separate individual plots of each country doesn't help, as 144 plots is too many. Making the lines transparent or fitting a simple model to all the data Figure \@ref(fig:spaghetti)B, might be a common first step to see common trends. However, all this seems to clarify is: 1) There is some set of countries that are similar, and they are distributed around the center of the countries, and 2) there is a general upward trend in heights over time. We learn about the collective, but lose sight of the individuals.

```{r spaghetti, fig.height = 4, fig.width = 12, message = FALSE, fig.cap = "The full dataset shown as a spaghetti plot (A), with transparency (B), and with a linear model overlayed (C). It is still hard to see the individuals.", echo = FALSE}
spaghetti <- ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line()

spaghetti_alpha <- ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line(alpha = 0.3)

spaghetti_smooth <- spaghetti_alpha + geom_smooth(method = "lm", 
                                                  aes(group = NULL))

spaghetti + spaghetti_alpha + spaghetti_smooth + 
  plot_annotation(tag_levels = "A")
```

This paper demonstrates how to effectively and efficiently explore longitudinal data, using the R package, `brolgar`. We examine four problems in exploring longitudinal data: 

1. How to sample the data
2. Finding interesting individuals
3. Finding representative individuals
4. Understanding a model

This paper proceeds in the following way: first, a brief review of existing approaches to longitudinal data (Section \@ref()), then the definition of longitudinal data (Section \@ref(data)),  then approaches to these four problems are discussed (Section (\@ref()) - Section (\@ref())), followed by a summary (\@ref()).

# Other approaches to longitudinal data {#other-approaches}

R provides basic time series, `ts`, objects, which are vectors or matrices that represent data sampled at equally spaced points in time. These have been extended through packages such as `xts`, and `zoo`, which only consider data in a wide format with a regular implied time series. These are not appropriate for longitudinal data, which can have indexes that are not time unit oriented, such as "Wave 1...2", or may contain irregular intervals.

Other packages focus more directly on panel data in R, focussing on data operations and model interfaces. The `pmdplyr` package provides "Panel Manoeuvres" in "dplyr" [@pmdplyr]. It defines the data structure in as a `pibble` object (**p**anel t**ibble**), requiring an `id` and `group` column being defined to identify the unique identifier and grouping. The `pmdplyr` package focusses on efficient and custom joins and functions, such as `inexact_left_join()`. It does not implement tidyverse equivalent tools, but instead extends their usecase with a new function, for example `mutate_cascade` and `mutate_subset`. `panelr` provides an interface for data reshaping on panel data, providing widening and lengthening functions (`widen_panel()` and `long_panel()` [@panelr]. It also provides model facilitating functions by providing its own interface for models mixed effects models. 

These software generally re-implement their own custom panel data class object, as well as custom data cleaning tasks, such as reshaping into long and wide form. They all share similar features, providing some identifying or index variable, and some grouping or key. 

(**not sure where to put this paragraph**)

Operations for custom long and wide formats of panel data are not needed since they are handled using `tsibble` and `dplyr`. This avoids needing to learn additional new functions that achieve the same reshaping results. There are indeed some useful aspects of the specific data cleaning, but they are limited in how they interact with other R packages. `brolgar` provides more flexibility since it relies on `tsibble`, which ensures data manipulation with `dplyr` respects groups. Additionally, it is not limited to specific panel data tools, and can work more generally with general modelling software, in particular other ecosystems such as `fable`  and `feasts`. 


# Longitudinal Data Structures {#data}

Longitudinal data goes by many names: Panel data, survey data, repeated measures, and time series, to name a few. Although there are small differences among these definitions related to data collection, context, and field, these data structures all share a fundamental similarity: they are measurements of the same individual over a time period. 

This time period has structure - the time component (dates, times, waves, seconds, etc), and the spacing between measurements - unequal or equal. This data structure needs to be respected during analysis, to avoid for example, collapsing across month when the data is collected every second, or assuming measurements occur at fixed time intervals. These mistakes can be avoided by encoding the data structure into the data itself. This information can then be accessed by analysis tools, providing a consistent way to understand and summarise the data. This ensures the different types of longitudinal data previously mentioned can be handled in the same way. 

## Building on `tsibble`

Because longitudinal data can be thought of as "individuals repeatedly measured through time", they can be considered as a type of time series, as defined in @fpp: "Anything that is observed sequentially over time **is a time series**". This definition has been realised as a time series `tsibble` in [@Wang2018]. These objects are defined as data meeting these conditions:

1. The `index`: the time variable
2. The `key`: variable(s) defining individual groups (or series)
3. 1 + 2 together determine a distinct row

If the specified key and index pair do not define a distinct row - for example, if there are duplicates in the data, the `tsibble` will not be created. This helps ensure the data is properly understood and cleaned before analysis is conducted, removing avoidable errors that might have impacted downstream decisions.

We can formally define our `heights` data from Figure \@ref(fig:heights-sample-plot) as a `tsibble` using, `as_tsibble`:


```{r define-heights, eval = FALSE}
heights <- as_tsibble(heights,
                      index = year,
                      key = country,
                      regular = FALSE)

```

The `index` is `year`, the `key` is `country`, and `regular = FALSE`, since there are not regular intervals in the years measured. Using a `tsibble` means they index and key time series information is recorded **once**, and can be referred to many times in other parts of the data analysis by time-aware tools.

In addition to providing consistent ways to manipulate time series data, further benefits to using `tsibble` are how it works within the `tidyverse` ecosystem, as well as the tidy time series packages called "tidyver**ts**", containing `fable` [@fable], `feasts`, [@feasts].  For example, `tsibble` provides `tidyverse` extension functions to explore implicit missing values in the `index` (e.g., `has_gaps()` and `fill_gaps()`), as well as grouping and partitioning based on the index with `index_by()`. For full details and examples of use with the tidyverts time series packages, see @Wang2018.

The `brolgar` package uses `tsibble` so users can take advantage of these tools, learning one way of operating a data analysis that will work and have overlap with other contexts.

## Characterising Individual Series

### Calculating a Feature

We can summarise the individual series by collapsing their many measurements into a single statistic, such as the minimum, maximum, or median, with one row per key. We do this with the `features` function from the `fabletools` package, made available in `brolgar`. This provides a summary of a given variable, accounting for the time series structure, and returning one row per key specified. It can be thought of as a time-series aware variant of the `summarise` function from `dplyr`. The `feature` function works by specifying the data, the variable to summarise, and the feature to calculate. A template is shown below

```r
features(<DATA>, <VARIABLE>, <FEATURE>)
```

or, with the pipe:

```r
<DATA> %>% features(<VARIABLE>, <FEATURE>)
```

For example, to calculate the minimum height for each key (country), in `heights`, we specify the `heights` data, then the variable to calculate features on, `height_cm`, then the feature to calculate, `min` (we write `c(min = min)` so the column calculated gets the name "min"):

```{r heights-summary}
heights_min <- features(.tbl = heights, 
                        .var = height_cm, 
                        features = c(min = min))

heights_min
```

We call these summaries `features` of the data.  We can use this information to summarise these features of the data, for example, visualising the distribution of minimum values (Figure \@ref(fig:gg-simple-features)A)

We are not limited to one feature at a time, many features can also be calculated, for example:

```{r heights-example-three-features}
heights_three <- heights %>%
  features(height_cm, c(
    min = min,
    median = median,
    max = max
  ))

heights_three
```

These can then be visualised together (Figure \@ref(fig:feature-min-med-max)).

```{r feature-min-med-max, fig.cap = "Three plots showing the distribution of minimum, median, and maximum values of height in cm. We see that there is overlap amongst all three.", echo = FALSE}
p1 <- ggplot(heights_min,
       aes(x = min)) + 
  geom_density(fill = "salmon") +
  theme(aspect.ratio = 1)

library(colorspace)
p2 <- heights_three %>% 
  pivot_longer(cols = min:max,
               names_to = "feature",
               values_to = "value") %>% 
  ggplot(aes(x = value,
             fill = feature)) + 
  geom_density(alpha = 0.5) +
  labs(x = "Value",
       y = "Density",
       fill = "Feature") + 
  scale_fill_discrete_qualitative(palette = "Dark 3") +
  # inset the legend?
  theme(legend.position = "none",
        aspect.ratio = 1)


p3 <- 
heights_three %>% 
  pivot_longer(cols = min:max,
               names_to = "feature",
               values_to = "value") %>% 
  mutate(feature = factor(feature,
                          levels = c("min",
                                     "median",
                                     "max"))) %>% 
ggplot(aes(x = feature,
           y = value,
           group = country)) + 
  geom_line(alpha = 0.6) +
  theme(aspect.ratio = 1)

p1 + p2 + p3
```


These sets of features can be pre-specified, for example, `brolgar` provides a five number summary (minimum, 25th quantile, median, mean, 75th quantile, and maximum) of the data with `feat_five_num`:

```{r features-fivenum, echo = TRUE}
heights_five <- heights %>%
  features(height_cm, feat_five_num)

heights_five
```

This takes the `heights` data, pipes it to `features`, and then tells it to summarise the `height_cm` variable, using `feat_five_num`.  There are several handy functions for calculating features of the data that 
`brolgar` provides. These all start with `feat_`, and additionally include:

- `feat_ranges()`: min, max, range difference, interquartile range; - 
- `feat_spread()`: variance, standard deviation, median absolute distance, and interquartile range; 
- `feat_monotonic()` is it always increasing, decreasing, or unvarying?
- `feat_diff_summary()`, the summary statistics of the differences amongst a value, including the five number summary, as well as the standard deviation and variance. 
- `feat_brolgar()`, which will calculate all features available in the `brolgar` package.
- Other examples of features from `feasts`

(Note: tie this section in below to this one)

You can use a prepared summary, for which `brolgar` provides a few - we've already seen `feat_diff_summary`, which provided a summary of the `diff` of a variable. `feat_ranges` produces range related features, minimum, maximum, difference from the range, and the interquartile range.

These are the ones that are already available:

- ...

```{r heights-features}
heights %>%
  features(height_cm, feat_ranges)

```


### Feature Sets

If you want to run many or all features from a package on your data you can collect them all with `feature_set`. For example:

```{r show-features-set}
library(fabletools)
feat_brolgar <- feature_set(pkgs = "brolgar")
length(feat_brolgar)
```

You could then run these like so: 

```{r run-features-set}
heights %>%
  features(height_cm, feat_brolgar)
```

For more information see `?fabletools::feature_set`

### Creating Your Own Feature

To create your own features or summaries to pass to `features`, you provide a named vector of functions. These can include functions that you have written yourself. For example, returning the first three elements of a series

```{r create-three, echo = TRUE}

second <- function(x) nth(x, n = 2)
third <- function(x) nth(x, n = 3)

feat_first_three <- c(first = first,
                      second = second,
                      third = third)

```

These are then passed to `features` like so:

```{r demo-feat-three, echo = TRUE}
heights %>%
  features(height_cm, feat_first_three)
```
 
 `brolgar` provides some useful additional features for the five number summary, `feat_five_num`, whether keys are monotonically increasing `feat_monotonic`, and measures of spread or variation, `feat_spread`. Inside `brolgar`, the features are created with the following syntax:

```{r demo-feat-five-num, eval = FALSE, echo = TRUE}
feat_five_num <- function(x, ...) {
  c(
    min = b_min(x, ...),
    q25 = b_q25(x, ...),
    med = b_median(x, ...),
    q75 = b_q75(x, ...),
    max = b_max(x, ...)
  )
}
```

Here the functions `b_` are functions with a default of `na.rm = TRUE`, and in 
the cases of quantiles, they use `type = 8`, and `names = FALSE`. What is particularly useful is that these will work on any type of time series data, and you can use other more typical time series features from the `feasts` package, such as autocorrelation, `feat_acf()`, Seasonal and Trend decomposition using Loess `feat_stl()` [@ref].

This demonstrates a workflow that can be used to understand and explore your longitudinal data. `brolgar` builds upon this workflow made available by `feasts` and `fabletools`. Users can also create their own features to summarise the data.

# Breaking up the Spaghetti

Plots like Figure \@ref(fig:spaghetti) are often called, "spaghetti plots", and can be useful for a high level understanding as a whole. However, we cannot process and understand the individuals when the data is presented like this.

## Sampling

Just how spaghetti is portioned out for consumption, we can sample some of the data by randomly sampling the data into sub-plots with the `facet_sample()` function  (Figure \@ref(fig:facet-sample)). 


```{r facet-sample, fig.cap = "Facet Samples"}
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_sample()
```

This defaults to 12 facets and 3 samples per facet, and provides options for the number of facets, and the number of samples per facet. This means the user only needs to consider the most relevant questions: "How many keys per facet?" and "How many facets to look at?". The code to change the figure from Figure \@ref(fig:spaghetti) into \@ref(fig:facet-sample) requires only one line of code, shown below:

```r
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_sample()
```


## Stratifying

Extending this idea of samples, we can instead look at **all** of the data, spread out equally over facets, with `facet_strata()`, with the default being 12 facets, controllable with `n_strata`. The code to do so is shown below, creating Figure \@ref(fig:facet-strata).

```{r facet-strata, echo = TRUE, fig.cap = "Stratified plot of heights data."}
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_strata()
```

## Featuring

Figure \@ref(fig:facet-strata) and Figure \@ref(facet-sample) only show each key once, being randomly assigned to a facet. We can meaningfully place the keys into facets, by arranging the heights "along" a variable, like `year`, using the `along` argument in `facet_strata` to produce Figure \@ref(fig:facet-year-along):

```{r facet-year-along, echo = TRUE}
heights %>% 
ggplot(aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_strata(along = -year)
```

Note we have not lost any of the data, and only the order they are presented has changed. We learn the distribution and changes in heights over time, and  those measured from the earliest times appear to be more similar, but there is much wider variation in the middle years, and then for more recent heights measured from the early 1900s, the heights are more similar. The starting point of each of these years seems to increase at roughly the same interval. This tells us the starting times of the years is roughly uniform.

Together `facet_sample()` and `facet_strata()` allow for rapid exploration, by focussing on relevant questions instead of the minutiae of appropriately randomly assigning while maintaining key structure, keeping the correct number of keys per plot, and so on. For example, `facet_sample()` the questions are: "How many lines per facet" and "How many facets?", and for `facet_strata()` the questions are: "How many facets / strata?" and "What to arrange plots along?". 

Answering these questions keeps the analysis in line with the analytic goals of exploring the data, rather than distracting away by moving attention to minutiae. This is a key theme of improving tools for data analysis. Abstracting away the parts that are not needed, so the analyst can focus on the task at hand.

Under the hood, `facet_sample()` and `facet_strata()` are powered with `sample_n_keys()` and `stratify_keys()`. These can be used to create data structures used in `facet_sample()` and `facet_strata()`, and extend them for other purposes.

Using a `tsibble` stores important key and index components, in turn allowing for better ways to break up spaghetti plots so we can look at many and all sub-samples using `facet_sample()` and `facet_strata()`.

# Book-keeping

Longitudinal data is not always measured at the same time and at the same frequency. When exploring longitudinal data, a useful first step is to explore the frequency of measurements of the index. We can check if the index is regular using `index_regular()` and summarise the spacing of the index with `index_summary()`. These are S3 methods, so for `data.frame` objects, the `index` must be specified, however for the `tsibble` objects, the defined index is used.

```{r index-summary-regular}
index_summary(heights)
index_regular(heights)
```

We can explore how many observations per country by counting the number of observations with `features`, like so:

```{r}
heights %>% features(year, n_obs)
```

This can be further summarised by counting the number of times there are a given number of observations:

```{r}
heights %>% features(year, n_obs) %>% count(n_obs)
```

Because we are exploring the temporal patterns, we cannot reliably say anything about those individuals with less than 5 measurements, so we can filter out those for further analysis. This uses `add_n_obs()` to add the number of observations to the existing data. This drops 25 countries, leaves us with 119 out of the original 144 countries.

```{r overwrite-heights}
heights <- heights %>% 
  add_n_obs() %>% 
  filter(n_obs >= 5)
```

We can further explore when countries are first getting measured using `features` to find the first year for each country number of starting years with the `first` function from `dplyr`, and explore this with a visualisation (Figure \@ref(fig:feature-first-gg)).

```{r feature-first}
heights %>% 
  features(year, c(first = first))
```

```{r feature-first-gg, fig.cap = "The starting years of measurement."}
heights %>% 
  features(year, c(first = first)) %>% 
  ggplot(aes(x = first)) +
  geom_bar() # data is already binned into 10 year blocks.
```

We can explore the variation in first year using `feat_diff_summary`. This combines many summaries of the differences in `year`.

```{r heights-diff-summary}
heights_diffs <- heights %>% 
  features(year, feat_diff_summary)

heights_diffs
```

This is particularly useful as using `diff` on `year` would return a very wide dataset that is hard to explore:

```{r diff-wide, message = FALSE, warning = FALSE}
heights %>% 
  features(year, diff)
```

We can then look at the summaries of the differences in year by changing to long form and facetting (Figure \@ref(fig:heights-long-feat-diff)), we learn about the range of intervals between measurements, the smallest being 10 years, the largest being 125, and that most of the data is measured between 10 and 30 years.

```{r heights-long-feat-diff, fig.cap = "Exploring the different summary statistics of the differences amongst the years. We learn that the smallest interval between measurements is 10 years, and the largest interval is between 10 and 125 years, and that most of the data is measured between 10 and 30 or so years.", echo = FALSE}
heights_diff_long <- heights_diffs %>% 
  pivot_longer(cols = -country,
               names_to = "feature",
               values_to = "value") %>% 
  mutate(feature = factor(feature,
                          levels = c("diff_min",
                                     "diff_q25",
                                     "diff_median",
                                     "diff_mean",
                                     "diff_q75",
                                     "diff_max",
                                     "diff_sd",
                                     "diff_var",
                                     "diff_iqr")))

ggplot(heights_diff_long,
       aes(x = value)) +
  geom_histogram(binwidth = 10,
                 colour = "gray") +
  facet_wrap(~feature,
             scales = "free")
```


# Finding Waldo

Looking at a spaghetti plot, it can be hard to identify which lines are the most interesting, or unusual. A workflow to identify interesting individuals starts with is given below:

1. Decide upon an interesting feature (e.g., maximum)
2. This feature produces one value per key
3. Examine the distribution of the feature
4. Join this table back to the data to get all observations for those keys
5. Arrange the keys or filter, using the feature
6. Display the data for selected keys

This workflow is demonstrated now.  Firstly, we **deicde on an interesting feature**, "maximum height", and whether height is always increasing. We calculate our own "feature", calculating maximum height, and whether a value is increasing (with brolgar's `increasing` function) like so:

```{r heights-max}
heights_max_in <- heights %>% 
  features(height_cm, list(max = max,
                           increase = increasing))

heights_max_in
```

This returns a dataset of **one value per key**. Figure \@ref(fig:heights-max-in-examine) **examines the distribution of the features**, showing us the distribution of maximum height, and the number of countries that are always increasing.

```{r heights-max-in-examine, fig.cap = "The different distributions of the features - A showing the distribution of maximum height, and B showing the number of countries that are always increasing (FALSE), and always increasing (TRUE). We note that the average maximum heights range from about 160cm to 185cm, with most being around 170cm. We also learn that vast majority of countries are not always increasing in height", echo = TRUE}

gg_max <- ggplot(heights_max_in,
       aes(x = max)) + 
  geom_histogram(binwidth = 1,
                 colour = "white") 

gg_inc <- ggplot(heights_max_in,
       aes(x = increase)) + 
  geom_bar()

gg_max + gg_inc + plot_annotation(tag_levels = "A")

```

We can now **Join this table back to the data to get all observations for those keys** to move from one key per row to all many rows per key.

```{r heights-max-in-full}
heights_max_in_full <- heights_max_in %>% 
  left_join(heights,
            by = "country")

heights_max_in_full
```

We can then **arrange the keys or filter, using the feature**, for example, filtering only those countries that are only increasing:

```{r heights-increase}
heights_increase <- heights_max_in_full %>% filter(increase)
heights_increase
```

Or tallest country

```{r heights-top-5}
heights_top <- heights_max_in_full %>% top_n(n = 1, wt = max)
heights_top
```

We can then display the data by highlighting it in the background, first creating a background plot

```{r gg-background}
gg_height_bg <- ggplot(heights_max_in_full,
       aes(x = year,
           y = height_cm,
           group = country)) +
  geom_line(colour = "gray",
            alpha = 0.3) 

gg_height_bg
```


```{r gg-increase-max}
(gg_height_bg + 
   geom_line(data = heights_increase)) +
(gg_height_bg + geom_line(data = heights_top)) +
  plot_annotation(tag_levels = "A")
```


# Dancing with Models

These same workflows can be used to interpret and explore a model. Let's take an example of the fixed effect of year and a random intercept for country, adding the predicted and residual values.

```{r heights-lmer}
# have a read of https://quantdev.ssri.psu.edu/tutorials/growth-modeling-chapter-3-linear-growth-models
# heights_fit <- nlme::nlme(model = height_cm ~ year,
#                           data = brolgar::heights,
#                           fixed = year ~ 1,
#                           random = country ~ 1,
#                           group = ~country,
#                           start = c(year=1800))
heights_fit <- lmer(height_cm ~ year + (1|country), data = brolgar::heights)
heights_aug <- brolgar::heights %>%
  add_predictions(heights_fit, var = "pred") %>%
  add_residuals(heights_fit, var = "res")
```

We can use the previous approach to explore the model results. We can take a look at a sample of the predictions along with the data, by using `sample_n_keys`. This provides a useful way to explore some set of the model predictions. In order to find those predictions that best summarise the best, and worst, and in between, we need to use the methods in the next section, "stereotyping".

```{r sample-n-keys-model, fig.cap = "Exploration of a random sample of the data against the predictions"}
heights_aug %>% 
  sample_n_keys(10) %>% 
  ggplot(aes(x = year,
             y = pred,
             group = country)) + 
  geom_line() +
  geom_point(aes(y = height_cm)) + 
  facet_wrap(~country)
```

# Stereotyping

To help understand a population of measurements over time, it can be useful to understand which individual measurements are typical (or "stereotypical") of a measurement. For example, understand which individuals are a stereotypical of a statistic such as the minimum, median, and maximum height. This section discusses how to find these stereotypes in data.

Figure \@ref(fig:heights-aug-res-summary) shows the residuals of the simple model fit to the data in the previous section. There is an overlaid five number summary, showing the minimum, 1st quantile, median, 3rd quantile, and maximum residual value residuals, as well as a rug plot to show the data. We can use these residuals to understand the stereotypes of the data - those individuals in the model that best match to this five number summary. 

```{r heights-aug-res-summary, fig.cap = "Five number summary of residual values", echo = FALSE}
heights_aug_res_summary <- 
tibble(names = names(summary(heights_aug$res)),
       values = as.numeric(round(summary(heights_aug$res),2))) %>% 
  slice(-4) %>% 
  mutate(y_pos = c(280, 275, 275, 250, 275))

gg_resid <- 
ggplot(heights_aug,
       aes(x = res)) + 
  geom_histogram(colour = "grey",
                 binwidth = 1) +
  # geom_density() +
  geom_rug(alpha = 0.2)

gg_resid + 
  geom_vline(xintercept = heights_aug_res_summary$values,
             colour = "salmon") + 
  ggrepel::geom_label_repel(data = heights_aug_res_summary,
             mapping = aes(label = names,
                           x = values,
                           y = y_pos))
```

We can do this using `keys_near()` from `brolgar`. By default this uses the 5 number summary, but any function can be used. You specify the variable you want to find the keys nearest, in this case `res`, the residuals:

```{r heights-aug-keys-near}
keys_near(heights_aug, 
          var = res)
```

To plot the data, they need to be joined back to the original data, we use a left join, joining by country.

```{r heights-keys-near-join}
heights_near_aug <- keys_near(heights_aug, 
          var = res) %>% 
  left_join(heights_aug,
            by = c("country"))
```

Figure \@ref(fig:heights-keys-near) shows those countries closest to the five number summary. Observing this, we see that the minimum residual for Ireland has a few outliers that throw the result off. This is useful, as it tells us that this model is not capturing the non-linear aspect of the data, and that such a model should be used to better represent the data.

```{r heights-keys-near}
ggplot(heights_near_aug,
       aes(x = year,
             y = pred,
             group = country,
             colour = country)) + 
  geom_line() + 
  geom_point(aes(y = height_cm)) + 
  facet_wrap(~stat + country,
             labeller = label_glue("Country: {country} \nNearest to {stat} residual")) + 
  theme(legend.position = "none")
```


We can also look at the nearest 3 countries to each statistics, by changing `top_n = 3`:

```{r heights-keys-near-extra}
heights_near_aug_3 <- keys_near(heights_aug, 
          var = res,
          top_n = 3) %>% 
  left_join(heights_aug,
            by = c("country"))
```

Figure \@ref(fig:heights-keys-near-fancy-label) shows the same information as the previous plot, but with the 3 representative countries for each statistic. This gives us more data on what the stereotypically "good" and "poor" fitting countries to this model.

```{r heights-keys-near-fancy-label, fig.height = 12, fig.cap = "Figure of stereotypes for three", echo = FALSE}
ggplot(heights_near_aug_3,
       aes(x = year,
             y = pred,
             group = country,
             colour = stat)) + 
  geom_line() + 
  geom_point(aes(y = height_cm)) + 
  facet_wrap(~stat + country,
             labeller = label_glue("Country: {country} \nNearest to {stat} residual"),
             ncol = 3) + 
  theme(legend.position = "none")
```


# How to Install and Getting Started

The `brolgar` R package can be installed from CRAN using `install.packages("brolgar")`. The functions are all designed to build upon existing packages, but are predicated on working with `tsibble`.  The package extends upon `ggplot2` to provide facets for exploration: `facet_sample()` and `facet_strata()`. Extending `dplyr`'s `sample_n()` and `sample_frac()` functions by providing sampling and stratifying based around keys: `sample_n_keys()`, `sample_frac_keys()`, and `stratify_keys()`. New functions are focussed around the use of `key`, for example `key_slope()` to find the slope of each key, and `keys_near()` to find those keys near a summary statistic. Finally, feature calculation is provided by building upon the existing time series feature package, `feasts`.

To get started with `brolgar` you must first ensure your data is specified as a `tsibble` - discussed earlier in the paper, there is also a vignette ["Longitudinal Data Structures"](http://brolgar.njtierney.com/articles/longitudinal-data-structures.html), which discusses these ideas. The next step we recommend is sampling some of your data with `facet_sample()`, and `facet_strata()`. When using `facet_strata()`, facets can be arranged in order of a variable, using the `along` argument, which can reveal interesting features. 

To further explore longitudinal data, we recommend finding summary features of each variable with `features`, and identifying variables that are near summary statistics, using `keys_near` to find individuals stereotypical of a statistical value.

# Summary 

The `brolgar` package facilitates exploring longitudinal data in R. It builds upon existing infrastructure from `tsibble`, and `feasts`, which work within the `tidyverse` family of R packages, as well as the newer, `tidyverts`, time series packages. Users familiar with either of these package families will find a lot of similarity in their use, and first time users will be able to easily transition from `brolgar` to the `tidyverse` or `tidyverts`. 

Future work will explore more features and stratifications, and stereotypes, and generalise the tools to work for data without time components.

# References {-}

<div id="refs"></div>

# Acknowledgements {-}

We'd like to thank Stuart Lee, Mitchell O'Hara Wild, Earo Wang, and Miles McBain for their discussion on the design of `brolgar`. We'd also like to thank Rob Hyndman, Monash University, and ACEMS for their supporting this research.

# Notes for fixing

- Keep it simple
- Remove large chunks of code
- No more overthinking! Just clean it up!
- Keep it simple
- Goal for next week, send it to Tania. 


# Extras

 First, looking at all the data (Figure \@ref(fig:gg-heights-lmer)A), then looking at many sub-samples with `facet_sample()` (Figure \@ref(fig:gg-heights-lmer)B), all sub-samples with `facet_strata()` (Figure \@ref(fig:gg-heights-lmer)C), all sub-samples, arranging along residuals, with `facet_strata(along = -res)` \@ref(fig:gg-heights-lmer)D). 

((**Note**: I'm not entirely sure how useful this is? I mean the last one is kinda useful?)

```{r gg-heights-lmer, fig.cap = "Figure of", fig.height = 10, fig.width = 6, echo = FALSE}
gg_heights_aug <- ggplot(heights_aug,
       aes(x = year,
           y = pred,
           group = country)) + 
  geom_line() 

p1 <- gg_heights_aug

p2 <- gg_heights_aug + facet_sample(n_facets = 6)

p3 <- gg_heights_aug + facet_strata(n_strata = 6)

p4 <- gg_heights_aug + facet_strata(n_strata = 6, along = -res)

p1 / p2 / (p3 + p4) + plot_annotation(tag_levels = "A")
```



**note - do I need this extra final paragraph, it feels a bit repetitive**

> We have addressed the four problem areas of, "looking at some of the data" by describing how to treat longitudinal data as a time series, and then using `facet_sample()` and `facet_strata()`. Interesting observations can be identified by first deciding what features are interesting, summarising observations to one key, deciding how to filter,r and then joining the summarised data back to the original. Models can be understood by applying the same tools used to explore data. 
