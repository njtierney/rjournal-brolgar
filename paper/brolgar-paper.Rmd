---
title: 'brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically
  in R'
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
  pdf_document:
    keep_tex: yes
---

<!-- --- -->
<!-- title: "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R" -->
<!-- author: -->
<!--   # see ?rjournal_article for more information -->
<!--   - name: Nicholas Tierney -->
<!--     affiliation: Monash University -->
<!--     address: -->
<!--     - Monash Clayton -->
<!--     url: https://njtierney.com -->
<!--     orcid: 0000-0002-9079-593X -->
<!--     email:  nicholas.tierney@gmail.com -->
<!--   - name: Dianne Cook -->
<!--     url: https://dicook.org -->
<!--     email: dicook@monash.edu -->
<!--     orcid: 0000-0002-9079-593X -->
<!--     affiliation: Monash University -->
<!--     address: -->
<!--     - Monash Clayton -->
<!--     affiliation2: ACEMS -->
<!--     address2: -->
<!--     - ACEMS Brisbane -->
<!-- abstract: > -->
<!--   An abstract of less than 150 words. -->
<!-- preamble: | -->
<!--   % Any extra LaTeX you need in the preamble -->

<!-- # per R journal requirement, the bib filename should be the same as the output  -->
<!-- # tex file. Don't forget to rename the bib file and change this example value. -->
<!-- bibliography: brolgar-paper.bib -->
<!-- output: rticles::rjournal_article -->
<!-- --- -->


```{r setup, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, message = FALSE}
library(tidyverse)
library(kableExtra)
library(lme4)
library(modelr)
library(tsibble)
library(brolgar)
library(patchwork)
library(conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("pull", "dplyr")
conflict_prefer("heights", "brolgar")
heights <- brolgar::heights %>% 
  filter(year > 1700)
# NOTE - how to discuss that the data has been filtered from original source?
```

# Abstract

Longitudinal (panel) data provide the opportunity to examine temporal patterns of individuals, because measurements are collected on the same person at different, and often irregular, time points. The data is typically visualised using a "spaghetti plot", where a line plot is drawn for each individual. When overlaid in one plot, it can have the appearance of a bowl of spaghetti. With even a small number of subjects, these plots are too overloaded to be read easily. The interesting aspects of individual differences are lost in the noise. Longitudinal data is often modeled with a hierarchical linear model to capture the overall trends, and variation among individuals, while accounting for various levels of dependence. However, these models can be difficult to fit, and can miss unusual individual patterns. Better visual tools can help to diagnose longitudinal models, and better capture the individual experiences. This paper introduces the R package, brolgar (BRowse over Longitudinal data Graphically and Analytically in R), which provides tools to identify and summarise interesting individual patterns in longitudinal data.

# Introduction

This paper is about exploring longitudinal data effectively. Longitudinal data provide the opportunity to examine temporal patterns of individuals, because measurements are collected on the same individual at different (often irregular), time points. Put another way, longitudinal data are "individuals repeatedly measured through time". Longitudinal data can be defined as individuals repeatedly measured through time. Table \@ref(tab:heights-sample) and Figure \@ref(fig:heights-sample-plot) show the individual as `country`, and `time` as year, with `country` being measured repeatedly, from 1900 to 1970. Also note there are irregular time periods between years.

```{r heights-sample}
heights_oz <- heights %>% filter(country == "Australia") %>%  slice_tail(n = 5) 
heights_oz %>% 
  kable(caption = "A sample of average heights of men in Australia for 1900-1970. The heights increase over time, but are measured at irregular intervals.")
```

```{r heights-sample-plot}
ggplot(heights_oz,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_point() + 
  geom_line()
# NOTE - add time points for each measurement?
```

The Australian data shown is just a sample, Figure \@ref(fig:spaghetti) shows the full heights dataset, of 144 countries. This plot is challenging to understand because there is overplotting, it makes it hard to see the individuals, and there are too many lines to look at individual plots of each country - looking at 144 plots will not help us. Making the lines transparent or fitting a simple model to all thje data (fig XX), might be a common first step to see common trends. However, all this seems to clarify is: 1) there is some set of countries that are more similar (in the center), and 2) there is a general upward trend in heights over time. These are useful things to know, but we lose sight of the individuals. For example, that the country with the greatest change in heights is XX, XX is the country with the tallest people, and XX has shortest people (Figure XX).

```{r spaghetti, fig.height = 4, fig.width = 12, message = FALSE}
spaghetti <- ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() 

spaghetti_alpha <- ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line(alpha = 0.3) 

spaghetti_smooth <- spaghetti_alpha + geom_smooth(method = "lm", 
                                                  aes(group = NULL))

spaghetti + spaghetti_alpha + spaghetti_smooth
```

```{r spaghetti-interesting-obs}
# code to identify: 
# For example, that the country with the greatest change in heights is XX, XX is the country with the tallest people, and XX has shortest people (Figure XX).
```

This paper addresses how to explore longitudinal data effectively, by examining four problems in exploring longitudinal data: 

1. How to look at some sample of the data
2. How to find interesting individuals
3. Finding representative individuals
4. How to understand a model

This paper proceeds in the following way: first, a brief review of the definition of longitudinal data, and existing approaches to longitudinal data, then approaches to these four problems are discussed, followed by a summary.

# Longitudinal Data Structures

There are many names of longitudinal data: Panel data, survey data, repeated measures, and time series, to name a few. Although there might be differences amongst these definitions, referring to data collection methods, or contexually (culturally?) how they are referred to within certain fields. These data structures all share a fundamental similarity.

To efficiently work with longitudinal data, we need a way to describe it that cuts across all these examples. Defining the data structure formally gives us a consistent way to understand and summarise the data. As defined earlier, longitudinal data can be thought of as "individuals repeatedly measured through time". This means that we can consider longitudinal data a type of time series, which is defined in @fpp: "Anything that is observed sequentially over time **is a time series**".

We can then formally define our `heights` data shown earlier as a `tsibble`, a time series object. To do so we must define three conditions:

1. The `index`: the time variable
2. The `key`: variable(s) defining individual groups (or series)
3. 1 + 2 together determine a distinct row

Referring to Table XX, `index` is `year`, the `key` is `country`. Defining a `tsibble` object is perfomed using `as_tsibble` below, specifying `regular = FALSE`, since there are not regular intervals in the years measured:


```{r define-heights, eval = FALSE}
heights <- as_tsibble(heights,
                      index = year,
                      key = country,
                      regular = FALSE)

heights
```

Using a `tsibblwe` means this important time series information is recorded **once**, and it will be automatically used many times in other parts of the data analysis.

# Other approaches to longitudinal data

## `pmdplyr`

https://cran.r-project.org/web/packages/pmdplyr
https://nickch-k.github.io/pmdplyr/index.html

Mostly concerns around doing effective  / custom joins, it provides its own data structure that is effectively equivalent to `tsibble`

## `panelr`

https://github.com/jacob-long/panelr
https://panelr.jacob-long.com/articles/wbm.html

Most of these packages provide more than just data definitions, they all share similar features, that there is some ID / index and some group / key feature. However, they also provide interfaces into things like model fitting (`panelr`) - this provides more tools around regression. It also provides widening and lengthening functions. This package fits a different niche to `brolgar`, instead providing helpers on modelling, and it's own interfaces into models. This is in opposition to brolgar, which provides instead workflow focussed helpers.

## `plm`

https://bookdown.org/ccolonescu/RPoE4/panel-data-models.html
http://www.mysmu.edu/faculty/anthonytay/Notes/Panel_in_R.pdf
https://cran.r-project.org/web/packages/plm/index.html
https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html

General comment on these papers. They generally reimplement their own custom panel data class object, as well as custom data cleaning tasks, such as reshaping into long and wide form. However, operations for custom long and wide form are not needed since they are handled within `tsibble` + `dplyr`. There are indeed some useful aspects of the specific data cleaning, but they are limited in how they interact with other R packages. `brolgar` provides more flexibility since it relies on `tsibble`, which ensures data manipulation with `dplyr` respects groups. Additionally, it is not limited to specific panel data tools, and can work more generally with general modelling software, in particular other ecosystems such as `fable`  and `feasts`. 

A lot of this is predicated on the fact that longitudinal data, panel data, repeated measures, and all those variations on the names of this dataset, can be thought of as being a type of time series. This is a unifying feature that helps bring the wider toolset from time series into the world longitudinal + panel data world.

# How to look at some sample of the data

Plots like Figure \@ref(fig:spaghetti) are often called, "spaghetti plots". These can be useful for a high level understand of the dataset, but we cannot process and understand the individuals. Just how spaghetti is portioned out and sampled individually to eat it, we can portion out these plots using a set of small subplots to create a plot like Figure \@ref(fig:facet-sample). 

```{r facet-sample, fig.cap = "Facet Samples"}
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_sample()
```

At a higher level, to create Figure \@ref(fig:facet-sample) all you need to create Figure \@ref(fig:facet-sample) is: the number of facets desired, and how many keys per facet. This involves randomly assigning a specified number of keys per subplot. In practice this results in answering other questions like: "how many keys are there", "how many keys per group", and then stratifying them into randomly assigned groups, ensuring the number of keys is kept the same, and managing edge cases when it does. This process is problematic in data analysis because answering each of these questions shifts attention from the analytic goal (a facetted plot of samples), ultimately distracting from the analysis. This is a key theme of improving tools for data analysis. Abstracting away the parts that are not needed, so the analyst can focus on the task at hand.

`brolgar` provides an approach that means the user only needs to consider the most relevant questions: "How many keys per facet?" and "How many facets to look at?", defaulting to 12 facets and three keys per facet. The code to create this figure is shown with:

```r
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_sample()
```

Extending this idea of samples, we can instead look at all of the data spread out equally over facets, using `facet_strata()` like so:

```{r facet-strata}
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_strata()
```

Here each key is shown only once, and is randomly assigned to a facet/subplot. We can provide meaning to each of these facets, by arranging the heights along a variable, like `year`, like so:

```{r facet-year-along}
heights %>% 
ggplot(aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_strata(along = -year)
```

(note on starting point being roughly uniform)
(note that we haven't lost any data - there are equal number of keys per plot as well)

```{r}
heights %>% 
  features(year, lst(first)) %>% 
  ggplot(aes(x = first)) +
  geom_bar() # data is already binned into 10 year blocks.
```

(show how we can batch up spaghetti, the question is to understand when are countries getting first measured)

(understanding how regularly the data are measured)

```{r}
heights_diffs <- heights %>% 
  features(year, list(n_obs,
                      diff)) %>%
  # maybe useful to have max_diff, min_diff, and other summaries
  #
  # features(year, max(diff(~.x))) %>%
  group_by(country) %>%
  mutate(max_diff = max(c_across(-n_obs), na.rm = TRUE),
         var_diff = var(c_across(-n_obs), na.rm = TRUE)) %>% 
  select(-starts_with("...")) 

heights_diffs %>% 
  ggplot(aes(x = max_diff)) +
  geom_histogram(binwidth = 10,
                 colour = "white")

heights_diffs %>% 
  ggplot(aes(x = var_diff)) +
  geom_histogram(colour = "white")
```


From this we learn the distribution and changes in heights over time, and that those mearsured from the earliest times appear to be more similar, but there is much wider variation in the middle years, and then for more recent heights measured from the early 1900s, the heights are more similar.

Together `facet_sample()` and `facet_strata()` allow for rapid exploration, by focussing on relevant questions instead of minutae. For example, `facet_sample()` the questions are: "How many lines per facet" and "How many facets?", and for `facet_strata()` the questions are: "How many facets / strata?" and "What to arrange plots along?". Under the hood, `facet_sample()` and `facet_strata()` are powered with `sample_n_keys()` and `stratify_keys()`, so the user can use these to create the data structures needed for these plots, and extend them to other purposes that we haven't considered.

We have addressed "how to look at some of the data" by using `tsibble` to store important data on the key and index components, which in turn allow us to look at many and all subsamples using `facet_sample()` and `facet_strata()`.

# How to find interesting individuals

A workflow to identify interesting individuals starts with: "what defines interesting?". In this case, let us suggest interesting is "tallest height", or "maximum height".

The workflow is represented in the figure XX below, which shows:

1. Identify features based on "interesting" metric (in this case max height)
2. This results in one observation per key
3. Identify from these features what is most interesting (e.g., the highest max height)
4. Filter the most interesting
5. Join this feature back to the data to get all observations for those keys
6. These can now be shown with the data, or separately.

(this will be a figure that combines slides 56 - 69 of https://njt-rstudio20.netlify.app/#85)

These features of interest are identified using the `features` function from `feasts` and `fabletools`, a tool built to work specifically with time series data. It provides a summary of a given variable, accounting for the time series structure, and returning one row per key specified. We can calculate our own "feature", calculating the maximum height like so:

```{r heights-max}
heights %>% 
  features(height_cm, list(min = min))
```

Where we pass `features` the variable we want to calculate a feature on, and a named list of features. You can use a prepared summary, for which `brolgar` provides a few. `feat_ranges`, to produce range related features, min, max, difference from the range, and the interquartile range.

```{r heights-features}
heights %>%
  features(height_cm, feat_ranges)

```

`brolgar` provides some useful additional features for the five number summary, `feat_five_num`, whether keys are monotonically increasing `feat_monotonic`, and measures of spread or variation, `feat_spread`. What is particularly great is that these will work on any type of time series data, and you can use other more classical time series features from `feasts`, such as autocorrelation, `feat_acf()`, Seasonal and Trend decomposition using Loess `feat_stl()`.

This demonstrates a workflow that can be used to understand and explore your longitudinal data. `brolgar` builds upon this workflow made available by `feasts` and `fabletools`. Users can also create their own features to summarise the data.

# Finding representative individuals

Using `key_slope` and `keys_near()`.

What if we grabbed a sample of those who have the best, middle, and worst residuals?

summary(heights_aug$res)

Which countries are nearest to these statistics?

keys_near(heights_aug, 
          var = res)
          
that best match the 5 number summary.

Join data back and explore

# How to understand a model

We now demonstrate how these workflows can be used to interpret and explore a model.
Let's take an example of the fixed effect of year + random intercept for country like so:

```{r heights-lmer}
heights_fit <- lmer(height_cm ~ year + (1|country), brolgar::heights)
heights_aug <- brolgar::heights %>%
  add_predictions(heights_fit, var = "pred") %>%
  add_residuals(heights_fit, var = "res")
```

We can use the previous approach to explore the data. First, looking at all the data:

then looking at many subsamples with `facet_sample()`, 

all subsamples with `facet_strata()`, 

all subsamples, arranging by residuals, with `facet_strata()`,

we can also look at the predictions with the data like so

# How to install and get started

(note: take a look at existing R package journal papers to see these examples)
(note: overview of function structure)
(goal for next week - content finished and paring down of paper to reasonable length.)

# Summary 

Problem #1: How do I look at some of the data?
Longitudinal data as a time series 
Specify structure, get a free lunch.
Look at as much of the raw data as possible
Use facet_sample() / facet_strata()

Problem #2: How do I find interesting observations?
Decide what features are interesting
Summarise down to one observation per key
Decide how to filter
Join this feature back to the data

Problem #3: How do I understand a model?
Look at (one, more or all!) subsamples
Arrange subsamples
(actually use similar approaches to earlier!)

Future work

* more features and stratification processes
* generalise to use for other structured data that doesn't have time components

# References {-}

<div id="refs"></div>

# Acknowledgements {-}

Di Cook
Tania Prvan
Stuart Lee
Mitchell O'Hara Wild
Earo Wang
Rob Hyndman
Miles McBain
Monash University
ACEMS

