---
title: "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R"
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
---

```{r setup}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, message = FALSE}
library(tidyverse)
library(brolgar)
library(kableExtra)
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("pull", "dplyr")
```

# Introduction

* This paper is about exploring longitudinal data effectively. 
* Longitudinal data can be defined as individuals repeatedly measured through time. Table \@ref(tab:heights-sample) and Figure \@ref(fig:heights-sample-plot) show the individual here is `country`, and the `time` is year, with `country` being measured repeatedly, from 1910 to 1970. Also not that there are irregular time periods between years.

```{r heights-sample}
heights_oz <- heights %>% filter(country == "Australia") %>%  slice_tail(n = 5) 
heights_oz %>% 
  kable(caption = "A sample of average heights of men in Australia for 1900-1970. The heights increase over time, but are measured at irregular intervals.")
```

```{r heights-sample-plot}
ggplot(heights_oz,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_point() + 
  geom_line()
```

The Australian data shown is just a sample, Figure \@ref(fig:spaghetti) shows the full heights dataset, of 144 countries. This plot is challenging to understand because there is overplotting, it makes it hard to see the individuals, and there are too many lines to look at individual plots of each country - looking at 144 plots will not help us. Making the lines transparent or fitting a simple model to all thje data (fig ...), might be a common first step to see common trends. However, all this seems to clarify is two things: 1) there is some set of countries that are more similar in the center of the mass, and 2) there is a general upward trend in heights over time. These are useful things to know, but we lose sight of the individuals. For example, that the country with the greatest change in heights is XX, XX is the country with the tallest people, and XX has shortest people (Figure XX).


```{r spaghetti}
spaghetti <- ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() 
spaghetti
```

```{r spaghetti-alpha}
spaghetti_alpha <- ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line(alpha = 0.3) 

spaghetti_alpha
```

```{r spaghetti-alpha-model}
spaghetti_alpha + geom_smooth(method = "lm",
                              aes(group = NULL))
```

```{r spaghetti-interesting-obs}
# code to identify: 
# For example, that the country with the greatest change in heights is XX, XX is the country with the tallest people, and XX has shortest people (Figure XX).
```

This paper addresses four problems in exploring longitudinal data: 

1. How to look at some sample of the data
2. How to find interesting individuals
3. Finding representative individuals
4. How to understand a model

These four problems are now discussed followed by a summary. However, first a brief review of the definition of longitudinal data, and existing approaches to longitudinal data are discussed.

# Longitudinal Data Structures

There are many names of longitudinal data: Panel data, survey data, repeated measures, and time series, to name a few. Although there might be differences amongst these definitions, referring to data collection methods, or contexually (culturally?) how they are referred to within certain fields. These data structures all share a fundamental similarity.

To efficiently work with longitudinal data, we need a way to describe it that cuts across all these examples. Defining the data structure formally gives us a consistent way to understand and summarise the data. As defined earlier, longitudinal data can be thought of as "individuals repeatedly measured through time". This means that we can consider longitudinal data a type of time series, which is defined in @fpp: "Anything that is observed sequentially over time **is a time series**".

We can then formally define our `heights` data shown earlier as a `tsibble`, a time series object. To do so we must define three conditions:

1. The `index`: the time variable
2. The `key`: variable(s) defining individual groups (or series)
3. 1 + 2 together determine a distinct row

Referring to Table XX, `index` is `year`, the `key` is `country`. Defining a `tsibble` object is perfomed using `as_tsibble` below, specifying `regular = FALSE`, since there are not regular intervals in the years measured:


```{r define-heights, eval = FALSE}
heights <- as_tsibble(heights,
                      index = year,
                      key = country,
                      regular = FALSE)

heights
```

Using a `tsibblwe` means this important time series information is recorded **once**, and it will be automatically used many times in other parts of the data analysis.

# Other approaches to longitudinal data

## `pmdplyr`

https://cran.r-project.org/web/packages/pmdplyr
https://nickch-k.github.io/pmdplyr/index.html

Mostly concerns around doing effective  / custom joins, it provides its own data structure that is effectively equivalent to `tsibble`

## `panelr`

https://github.com/jacob-long/panelr
https://panelr.jacob-long.com/articles/wbm.html

Most of these packages provide more than just data definitions, they all share similar features, that there is some ID / index and some group / key feature. However, they also provide interfaces into things like model fitting (`panelr`) - this provides more tools around regression. It also provides widening and lengthening functions. This package fits a different niche to `brolgar`, instead providing helpers on modelling, and it's own interfaces into models. This is in opposition to brolgar, which provides instead workflow focussed helpers.

## `plm`

https://bookdown.org/ccolonescu/RPoE4/panel-data-models.html
http://www.mysmu.edu/faculty/anthonytay/Notes/Panel_in_R.pdf
https://cran.r-project.org/web/packages/plm/index.html
https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html

General comment on these papers. They generally reimplement their own custom panel data class object, as well as custom data cleaning tasks, such as reshaping into long and wide form. However, operations for custom long and wide form are not needed since they are handled within `tsibble` + `dplyr`. There are indeed some useful aspects of the specific data cleaning, but they are limited in how they interact with other R packages. `brolgar` provides more flexibility since it relies on `tsibble`, which ensures data manipulation with `dplyr` respects groups. Additionally, it is not limited to specific panel data tools, and can work more generally with general modelling software, in particular other ecosystems such as `fable`  and `feasts`. 

A lot of this is predicated on the fact that longitudinal data, panel data, repeated measures, and all those variations on the names of this dataset, can be thought of as being a type of time series. This is a unifying feature that helps bring the wider toolset from time series into the world longitudinal + panel data world.

# How to look at some sample of the data

Plots like Figure XX are often called, "spaghetti plots". These can be useful for a high level understand of the dataset, but we cannot process and understand the individuals. Just how spaghetti is portioned out and sampled individually to eat it, we can portion out these plots using a set of small subplots to create a plot like Figure \@ref(fig:facet-sample). 

```{r facet-sample, fig.cap = "Facet Samples".}
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_sample()
```

At a higher level, to create Figure \@ref(fig:facet-sample) you need to understand the number of keys in the data, the number of facets in the desired graphic, and how many keys per facet to draw. 

Implementing this new data structure becomes challenging as it is hard to address details such as keeping the same number of keys per subplot, as well as other minutae. This is problematic in data analysis because answering each of these questions shifts attention from the analytic goal (the graphic), ultimately distracting from the analysis. `brolgar` provides an approach that means the user only needs to consider two questions: "How many keys per facet?" and "How many facets to look at?", defaulting to 12 facets and three keys per facet. The code to create this figure is shown with:

```r
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_sample()
```

Extending this idea of subsamples, we can instead look at all of the data spread out equally over facets, using `facet_strata()` like so:

```{r facet-strata}
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_strata()
```

Here each key is shown only once, and is randomly assigned to a facet/subplot. We can provide meaning to each of these facets, by arranging the heights along a variable, like `year`, like so:

```{r facet-year-along}
ggplot(heights,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_strata(along = -year)
```

Together `facet_sample()` and `facet_strata()` allow for rapid exploration, by focussing on relevant questions instead of minutae. For example, `facet_sample()` the questions are: "How many lines per facet" and "How many facets?", and for `facet_strata()` the questions are: "How many facets / strata?" and "What to arrange plots along?". Under the hood, `facet_sample()` and `facet_strata()` are powered with `sample_n_keys()` and `stratify_keys()`, so the user can use these to create the data structures needed.

We have addressed "how to look at some of the data" by using `tsibble` to store important data on the key and index components, which in turn allow us to look at many and all subsamples using `facet_sample()` and `facet_strata()`.

# How to find interesting individuals

We propose a workflow to identify interesting individuals. This starts by asking "what defines interesting?". In this case, let us suggest interesting is "tallest height", or "maximum height".

The workflow is represented in the figure below, which shows:

1. Identify features based on "interesting" metric (in this case max height)
2. This results in one observation per key
3. Identify from these features what is most interesting (e.g., the highest max height)
4. Filter the most interesting
5. Join this feature back to the data to get all observations for those keys
6. These can now be shown with the data, or separately.

(this will be a figure that combines slides 56 - 69 of https://njt-rstudio20.netlify.app/#85)

These features of interest are identified using the `features` function from `feasts` and `fabletools`. 

```{r heights-features}
heights %>%
  features(height_cm, feat_five_num)
```

`brolgar` provides some useful additional features:

* feat_ranges
* feat_monotonic
* feat_spread

And still more features such as `feat_acf()` and `feat_stl()` from `feasts` can be used.

This demonstrates a workflow that can be used to understand and explore your longitudinal data. `brolgar` builds upon this workflow made available by `feasts` and `fabletools`. Users can also create their own features to summarise the data.

# Finding representative individuals

Using `key_slope` and `keys_near()`.

What if we grabbed a sample of those who have the best, middle, and worst residuals?

summary(heights_aug$res)

Which countries are nearest to these statistics?

keys_near(heights_aug, 
          var = res)
          
üîë üîë üîë that best match the 5 number summary.

Join data back and explore

# How to understand a model

We now demonstrate how these workflows can be used to interpret and explore a model.
Let's take an example of the fixed effect of year + random intercept for country like so:

```{r heights-lmer}
heights_fit <- lmer(height_cm ~ year + (1|country), heights)
heights_aug <- heights %>%
  add_predictions(heights_fit, var = "pred") %>%
  add_residuals(heights_fit, var = "res")
```

We can use the previous approach to explore the data. First, looking at all the data:

then looking at many subsamples with `facet_sample()`, 

all subsamples with `facet_strata()`, 

all subsamples, arranging by residuals, with `facet_strata()`,

we can also look at the predictions with the data like so


# Summary 

Problem #1: How do I look at some of the data?
Longitudinal data as a time series üíπ
Specify structure, get a free lunch. ü•™
Look at as much of the raw data as possible üç£
Use facet_sample() / facet_strata()

Problem #2: How do I find interesting observations?
Decide what features are interesting
Summarise down to one observation per key
Decide how to filter
Join this feature back to the data

Problem #3: How do I understand a model?
Look at (one, more or all!) subsamples
Arrange subsamples
(actually use similar approaches to earlier!)

Future work

* more features and stratification processes
* generalise to use for other structured data that doesn't have time components


Acknowledgements

Di Cook
Tania Prvan
Stuart Lee
Mitchell O'Hara Wild
Earo Wang
Rob Hyndman
Miles McBain
Monash University
ACEMS


  * Issues with longitudinal data
    * spaghetti plots + hierarchical modelling
    * we lose information on the individual
  * identifying the individual experience amongst the collective mass is hard FIGURE - SPAGHETTI PLOT
    * why is it hard?
    * we don't see the individual
    * overplotting
    * identifying interesting observations or useful subplot sizes increases chances of a **distraction threshold** being reached
      * transparency doesn't really help
    * identifying a subplot size
    * Because identifying what is "interesting" involves a few steps that can easily be lost as the problem is dealt with.
* We need methods to understand the individuals and the individual experience
* `brolgar` provides a set of methods for exploring individual experience in 
  longitudinal data.
* The paper proceeds in four sections
  1. Mechanics
  1. How to look at sample of the data
  1. How to identify interesting observations
  1. how to understand a model
1. Mechanics
    1. Data structures (`tsibble`) - from ["data structures" vignette](http://brolgar.njtierney.com/articles/longitudinal-data-structures.html) - @njtierney 
        - what is longitudinal data
    1. Calculations (`features` + `key_slope`) []
    1. [Calculating Features](http://brolgar.njtierney.com/articles/finding-features.html) - @njtierney 
       - outline the different things we can create.
       - explain what can be calculated, and what is available that is being calculated.
       - How to create your own (e.g., mean(diff(x) as opposed to `increasing`
       - (1. [Exploratory Modelling](http://brolgar.njtierney.com/articles/exploratory-modelling.html) - @njtierney (really this is identifying features - since slope is just another feature that we can use, reduce this section / find overlap with "finding features").)
1. Explorations
    1. [Finding interesting observations](http://brolgar.njtierney.com/articles/id-interesting-obs.html) - @njtierney 
    1. [Summarising individuals]() - keys_near and colour palettes (use of `palap`).
    1. [Diagnosing models](http://brolgar.njtierney.com/articles/mixed-effects-models.html) - @njtierney 
1. Summary + Discussion
